{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Student Information\n",
    "Name: Yu-Han Zhao 趙宇涵\n",
    "\n",
    "Student ID: 110033635\n",
    "\n",
    "GitHub ID: honey0703\n",
    "\n",
    "Kaggle name: honey0703\n",
    "\n",
    "Kaggle private scoreboard snapshot:\n",
    "\n",
    "[Snapshot](img/pic0.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. First: __This part is worth 30% of your grade.__ Do the **take home** exercises in the [DM2021-Lab2-master Repo](https://github.com/fhcalderon87/DM2021-Lab2-master). You may need to copy some cells from the Lab notebook to this notebook. \n",
    "\n",
    "\n",
    "2. Second: __This part is worth 30% of your grade.__ Participate in the in-class [Kaggle Competition](https://www.kaggle.com/c/dm2021-lab2-hw2/) regarding Emotion Recognition on Twitter. The scoring will be given according to your place in the Private Leaderboard ranking: \n",
    "    - **Bottom 40%**: Get 20% of the 30% available for this section.\n",
    "\n",
    "    - **Top 41% - 100%**: Get (60-x)/6 + 20 points, where x is your ranking in the leaderboard (ie. If you rank 3rd your score will be (60-3)/6 + 20 = 29.5% out of 30%)   \n",
    "    Submit your last submission __BEFORE the deadline (Dec. 24th 11:59 pm, Friday)__. Make sure to take a screenshot of your position at the end of the competition and store it as '''pic0.png''' under the **img** folder of this repository and rerun the cell **Student Information**.\n",
    "    \n",
    "\n",
    "3. Third: __This part is worth 30% of your grade.__ A report of your work developping the model for the competition (You can use code and comment it). This report should include what your preprocessing steps, the feature engineering steps and an explanation of your model. You can also mention different things you tried and insights you gained. \n",
    "\n",
    "\n",
    "4. Fourth: __This part is worth 10% of your grade.__ It's hard for us to follow if your code is messy :'(, so please **tidy up your notebook** and **add minimal comments where needed**.\n",
    "\n",
    "\n",
    "Upload your files to your repository then submit the link to it on the corresponding e-learn assignment.\n",
    "\n",
    "Make sure to commit and save your changes to your repository __BEFORE the deadline (Dec. 29th 11:59 pm, Wednesday)__. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Preparation\n",
    "### 1.1 Load data\n",
    "1. Read json files\n",
    "2. Split to train and test df\n",
    "3. Append emotions after traon df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter setting\n",
    "featureNumber = 20000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read json data to pd\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# df = pd.read_json(\"dm2021-lab2-hw2/tweets_DM.json\",lines=True, orient='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Show df\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # select '_source' column as 'source'\n",
    "# source = df._source\n",
    "\n",
    "# # normalize 'source'\n",
    "# twit = pd.json_normalize(source)\n",
    "\n",
    "# # show twit\n",
    "# twit.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # rename column for merge later\n",
    "\n",
    "# twit = twit.rename(columns={\"tweet.hashtags\": \"hashtags\", \"tweet.tweet_id\": \"tweet_id\", \"tweet.text\": \"text\"})\n",
    "# twit.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # to split train test\n",
    "# # read file 'data_identification.csv'\n",
    "\n",
    "# iden = pd.read_csv('dm2021-lab2-hw2/data_identification.csv')\n",
    "# iden.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # merge twit dataframe with data_identification.csv\n",
    "\n",
    "# total = pd.merge(twit, iden, on=\"tweet_id\", how=\"left\")\n",
    "# total.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # categorize the data into two set, train and test.\n",
    "\n",
    "# train = total[total[\"identification\"] == \"train\"]\n",
    "# test = total[total[\"identification\"] == \"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # drop hashtags column and identification column, since we will not use it\n",
    "\n",
    "# train = train.drop(columns=['hashtags', 'identification'])\n",
    "# test = test.drop(columns=['hashtags', 'identification'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # read the labels\n",
    "# emo = pd.read_csv('dm2021-lab2-hw2/emotion.csv')\n",
    "# emo.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # merge train with emotions.csv\n",
    "\n",
    "# train = pd.merge(train, emo, on=\"tweet_id\", how=\"left\")\n",
    "# train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # see the shape of the data\n",
    "\n",
    "# print(\"train shape :\", train.shape)\n",
    "# print(\"test shape :\", test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Save data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save to pickle file\n",
    "\n",
    "# train.to_pickle(\"train_df.pkl\") \n",
    "# test.to_pickle(\"test_df.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1455563, 3)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load a pickle file\n",
    "\n",
    "train_df = pd.read_pickle(\"train_df.pkl\")\n",
    "test_df = pd.read_pickle(\"test_df.pkl\")\n",
    "train_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Exploratory data analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#group to find distribution\n",
    "\n",
    "# train_df.groupby(['emotion']).count()['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib inline\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # the histogram of the data\n",
    "# labels = train_df['emotion'].unique()\n",
    "# post_total = len(train_df)\n",
    "# df1 = train_df.groupby(['emotion']).count()['text']\n",
    "# df1 = df1.apply(lambda x: round(x*100/post_total,3))\n",
    "\n",
    "# #plot\n",
    "# fig, ax = plt.subplots(figsize=(10,5))\n",
    "# plt.bar(df1.index,df1.values)\n",
    "\n",
    "# #arrange\n",
    "# plt.ylabel('% of instances')\n",
    "# plt.xlabel('Emotion')\n",
    "# plt.title('Emotion distribution')\n",
    "# plt.grid(True)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature Engineering\n",
    "### 2.0 Sample and train/val split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into training and testing set\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = train_df['text']\n",
    "y = train_df['emotion']\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.20, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # see the shape of the data\n",
    "\n",
    "# print('X_train', X_train.shape)\n",
    "# print('y_train', y_train.shape)\n",
    "# print('X_val', X_val.shape)\n",
    "# print('y_val', y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Using Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "def BOW():\n",
    "    # build analyzers (bag-of-words)\n",
    "    BOW_vectorizer = CountVectorizer() \n",
    "    \n",
    "    # 1. Learn a vocabulary dictionary of all tokens in the raw documents.\n",
    "    BOW_vectorizer.fit(X_train)\n",
    "\n",
    "    # 2. Transform documents to document-term matrix.\n",
    "    train_data_BOW_features = BOW_vectorizer.transform(X_train)\n",
    "    val_data_BOW_features = BOW_vectorizer.transform(X_val)\n",
    "    \n",
    "    #  add .toarray() to show\n",
    "    train_data_BOW_features.toarray()\n",
    "    \n",
    "    # check the dimension\n",
    "    print ('shape: ', train_data_BOW_features.shape)\n",
    "    \n",
    "    # observe some feature names\n",
    "    feature_names = BOW_vectorizer.get_feature_names()\n",
    "    print (feature_names[100:110])\n",
    "    print (\"😂\" in feature_names)\n",
    "    return BOW_vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Use nltk tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "def NLTK():\n",
    "    # build analyzers (bag-of-words)\n",
    "    BOW_500 = CountVectorizer(max_features=featureNumber, tokenizer=nltk.word_tokenize) \n",
    "\n",
    "    # apply analyzer to training data\n",
    "    BOW_500.fit(X_train)\n",
    "\n",
    "    train_data_BOW_features_500 = BOW_500.transform(X_train)\n",
    "\n",
    "    ## check dimension\n",
    "    print ('shape:', train_data_BOW_features_500.shape)\n",
    "    \n",
    "    train_data_BOW_features_500.toarray()\n",
    "    \n",
    "    # observe some feature names\n",
    "    feature_names_500 = BOW_500.get_feature_names()\n",
    "    print (feature_names_500[:10])\n",
    "    print (\"😂\" in feature_names)\n",
    "    return BOW_500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def TFIDF():\n",
    "    TFIDF_500 = TfidfVectorizer(max_features=featureNumber, tokenizer=nltk.word_tokenize, stop_words='english')\n",
    "    TFIDF_500.fit(X_train)\n",
    "#     train_data_TFIDF_features_500 = TFIDF_500.transform(X_train)\n",
    "#     print ('shape: ', train_data_TFIDF_features_500.shape)\n",
    "#     train_data_TFIDF_features_500.toarray()\n",
    "#     feature_names_500 = TFIDF_500.get_feature_names()\n",
    "#     print (feature_names_500[:10])\n",
    "#     print (\"😂\" in feature_names_500)\n",
    "    return TFIDF_500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zhaoyuhan/opt/anaconda3/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:489: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
     ]
    }
   ],
   "source": [
    "# Choose the type of features\n",
    "featureType = TFIDF()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose feature (BOW_500 / TFIDF_500)\n",
    "# featureType = BOW_500 \n",
    "\n",
    "# for a classificaiton problem, you need to provide both training & testing data\n",
    "X_train_f = featureType.transform(X_train)\n",
    "X_val_f = featureType.transform(X_val)\n",
    "\n",
    "# for whole dataset\n",
    "X_f = featureType.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape:  (1164450, 20000)\n",
      "y_train.shape:  (1164450,)\n",
      "X_val.shape:  (291113, 20000)\n",
      "y_val.shape:  (291113,)\n"
     ]
    }
   ],
   "source": [
    "## take a look at data dimension is a good habbit  :)\n",
    "print('X_train.shape: ', X_train_f.shape)\n",
    "print('y_train.shape: ', y_train.shape)\n",
    "print('X_val.shape: ', X_val_f.shape)\n",
    "print('y_val.shape: ', y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "def DecisionTree():\n",
    "    ## build DecisionTree model\n",
    "    DT_model = DecisionTreeClassifier(random_state=0)\n",
    "\n",
    "    ## training!\n",
    "    DT_model = DT_model.fit(X_train_f, y_train)\n",
    "\n",
    "    ## predict!\n",
    "    y_train_pred = DT_model.predict(X_train_f)\n",
    "    y_val_pred = DT_model.predict(X_val_f)\n",
    "\n",
    "    ## so we get the pred result\n",
    "    print (y_val_pred[:10])\n",
    "    return y_train_pred, y_val_pred, DT_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* with all training_split data, 500 TFIDF features.  \n",
    "training accuracy: 0.89  \n",
    "testing accuracy: 0.42  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "def NaiveBayes():\n",
    "    NB_model = MultinomialNB()\n",
    "    \n",
    "    # training!\n",
    "    NB_model = NB_model.fit(X_train_f, y_train)\n",
    "    \n",
    "    ## predict!\n",
    "    y_train_pred = NB_model.predict(X_train_f)\n",
    "    y_val_pred = NB_model.predict(X_val_f)\n",
    "\n",
    "    ## so we get the pred result\n",
    "    print (y_val_pred[:10])\n",
    "    \n",
    "    return y_train_pred, y_val_pred, NB_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "def RandomForest():\n",
    "    RF_model = RandomForestClassifier(criterion='entropy', n_estimators=10, random_state=0, n_jobs= -1,\n",
    "                                    max_depth=11, max_features='log2', verbose=1)\n",
    "    \n",
    "    # Training\n",
    "    RF_model = RF_model.fit(X_train_f, y_train)\n",
    "    \n",
    "    # Tesing\n",
    "    y_train_pred = RF_model.predict(X_train_f)\n",
    "    y_val_pred = RF_model.predict(X_val_f)\n",
    "    \n",
    "    # so we get the pred result\n",
    "    print (y_val_pred[:10])\n",
    "    \n",
    "    return y_train_pred, y_val_pred, RF_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* with all training_split data, 500 TFIDF features:  \n",
    "training accuracy: 0.39  \n",
    "testing accuracy: 0.38  \n",
    "tend to predict all to \"joy\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "def Logistic():\n",
    "    LR_model = LogisticRegression(C=1000, penalty=\"l2\")\n",
    "    \n",
    "    # Training\n",
    "    LR_model = LR_model.fit(X_train_f, y_train)\n",
    "    \n",
    "    # Testing\n",
    "    y_train_pred = LR_model.predict(X_train_f)\n",
    "    y_val_pred = LR_model.predict(X_val_f)\n",
    "    \n",
    "    # so we get the pred result\n",
    "    print (y_val_pred[:10])\n",
    "    \n",
    "    return y_train_pred, y_val_pred, LR_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* with all triaining_split data, 2000 TFIDF features:  \n",
    "training accuracy: 0.51  \n",
    "testing accuracy: 0.51\n",
    "* with all training_split data, 5000 TFIDF features:  \n",
    "training accuracy: 0.53  \n",
    "testing accuracy: 0.52  \n",
    "kaggle: 0.42992\n",
    "* with all training_split data, 20000 TFIDF features:  \n",
    "training accuracy: 0.55  \n",
    "testing accuracy: 0.54  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "def MLP():\n",
    "    NN_model = MLPClassifier(activation='tanh', solver='sgd', max_iter=100,\n",
    "                             hidden_layer_sizes=(256,64,16), random_state=None, verbose=True,\n",
    "                             early_stopping=False, tol=0.0001)\n",
    "    \n",
    "    # Training \n",
    "    NN_model = NN_model.fit(X_train_f, y_train)\n",
    "    \n",
    "    # Testing\n",
    "    y_train_pred = NN_model.predict(X_train_f)\n",
    "    y_val_pred = NN_model.predict(X_val_f)\n",
    "    \n",
    "    # so we get the pred result\n",
    "    print (y_val_pred[:10])\n",
    "    \n",
    "    return y_train_pred, y_val_pred, NN_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* TFIDF512, teanh, sgd, e30, (256,64,16)  \n",
    "training accuracy: 0.47  \n",
    "testing accuracy: 0.47  \n",
    "* TFIDF512, teanh, sgd, e100, (256,64,16)  \n",
    "training accuracy: 0.49  \n",
    "testing accuracy: 0.48\n",
    "* TFIDF1024, teanh, sgd, e100, (256,64,16)  \n",
    "training accuracy: 0.51  \n",
    "testing accuracy: 0.51\n",
    "* TFIDF20000, teanh, sgd, e100, (256,64,16)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.n Choose Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.73491689\n"
     ]
    }
   ],
   "source": [
    "# Choose classifier\n",
    "y_train_pred, y_val_pred, model = MLP()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # save model\n",
    "    import pickle\n",
    "    f = open('Model/NNmodel_sgd_tanh_e100_TF20000.pickle', 'wb')\n",
    "#     f = open('Model/RLmodel_TF512.pickle', 'wb')\n",
    "    pickle.dump(model, f)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## accuracy\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "acc_train = accuracy_score(y_true=y_train, y_pred=y_train_pred)\n",
    "acc_val = accuracy_score(y_true=y_val, y_pred=y_val_pred)\n",
    "\n",
    "print('training accuracy: {}'.format(round(acc_train, 2)))\n",
    "print('testing accuracy: {}'.format(round(acc_val, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## precision, recall, f1-score,\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_true=y_val, y_pred=y_val_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## check by confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cm = confusion_matrix(y_true=y_val, y_pred=y_val_pred) \n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funciton for visualizing confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "def plot_confusion_matrix(cm, classes, title='Confusion matrix',\n",
    "                          cmap=sns.cubehelix_palette(as_cmap=True)):\n",
    "    \"\"\"\n",
    "    This function is modified from: \n",
    "    http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n",
    "    \"\"\"\n",
    "    classes.sort()\n",
    "    tick_marks = np.arange(len(classes))    \n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(8,8))\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           xticklabels = classes,\n",
    "           yticklabels = classes,\n",
    "           title = title,\n",
    "           xlabel = 'True label',\n",
    "           ylabel = 'Predicted label')\n",
    "\n",
    "    fmt = 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt), horizontalalignment=\"center\", color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    ylim_top = len(classes) - 0.5\n",
    "    plt.ylim([ylim_top, -.5])\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot your confusion matrix\n",
    "my_tags = ['anger', 'anticipation', 'disgust', 'fear', 'sadness', 'surprise', 'trust', 'joy']\n",
    "plot_confusion_matrix(cm, classes=my_tags, title='Confusion matrix')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_test.shape:  (411972, 20000)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "X has 20000 features, but MLPClassifier is expecting 1024 features as input.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-91-0e1df80474a9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Predict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0my_test_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0my_test_pred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m   1032\u001b[0m         \"\"\"\n\u001b[1;32m   1033\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1034\u001b[0;31m         \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_pass_fast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1035\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py\u001b[0m in \u001b[0;36m_forward_pass_fast\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    132\u001b[0m             \u001b[0mThe\u001b[0m \u001b[0mdecision\u001b[0m \u001b[0mfunction\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0meach\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m         \"\"\"\n\u001b[0;32m--> 134\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'csr'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'csc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0;31m# Initialize first layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    435\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    436\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcheck_params\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ensure_2d'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 437\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_n_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    438\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_check_n_features\u001b[0;34m(self, X, reset)\u001b[0m\n\u001b[1;32m    363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn_features\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_features_in_\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 365\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    366\u001b[0m                 \u001b[0;34mf\"X has {n_features} features, but {self.__class__.__name__} \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    367\u001b[0m                 f\"is expecting {self.n_features_in_} features as input.\")\n",
      "\u001b[0;31mValueError\u001b[0m: X has 20000 features, but MLPClassifier is expecting 1024 features as input."
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "X_test = test_df['text']\n",
    "\n",
    "# Transform X\n",
    "X_test = featureType.transform(X_test)\n",
    "print('X_test.shape: ', X_test.shape)\n",
    "\n",
    "# Predict\n",
    "y_test_pred = model.predict(X_test)\n",
    "print (y_test_pred[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Submission\n",
    "submission = pd.DataFrame({'id':test_df['tweet_id'],'emotion':y_test_pred})\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the shape\n",
    "submission.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'Result/'\n",
    "filename = 'NN_TFIDF20000.csv'\n",
    "submission.to_csv(path+filename, index=False)\n",
    "print('Saved file: ' + filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
